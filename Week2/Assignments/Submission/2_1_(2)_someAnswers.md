### (1)沧海一声喵 
朴素贝叶斯：朴素贝叶斯方法是基于贝叶斯定理和特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率(Maximum A Posteriori)最大的输出y。

“朴素”指的是条件独立型假设，即条件概率假设各个条件相互独立。即不同特征之间是相互独立的。

### (2)九桢望乡 
原理:朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练集，首先基于特征条件独立假设学习输入、输出的联合概率分布；基于此模型，对给定输入x，利用贝叶斯定理求出后验概率最大的输出y。

优点：在数据较少的情况下仍然有效，可以处理多类别问题。
缺点：对于输入数据的准备方式较为敏感。
适用数据类型：标称型数据。

“朴素”是因为整个实现过程只基于朴素贝叶斯法所做最原始、最简单的假设。

### (3)蓝石头河 
朴素贝叶斯是一种基于概率理论的分类算法。最核心的部分是贝叶斯法则，而贝叶斯法则的基石是条件概率。叶斯法则为p(ci|x,y)=p(x,y|ci)p(ci)/p(x,y)。ci表示类别i，输入待判断数据，式子给出要求解的某一类的概率，比较各类别的概率值大小，选择最大概率对应的类别作为预测类别。

朴素贝叶斯分类器基于一个简单的假定：给定目标值时属性之间相互条件独立。该假定说明给定实例的目标值情况下。观察到联合的a1,a2...an的概率正好是对每个单独属性的概率乘积。

### (4)立方体的太阳 
朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。这种条件独立性的假设就是朴素贝叶斯法“朴素”二字的由来。
对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布，然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。

### (5)宸羽 
基于训练集，生成先验概率，条件概率，得到贝叶斯分类模型。对于给定的输入x，通过模型计算最大的后验概率y，得到分类结果。其中，如果各个属性对于分类结果的影响不是相互独立的，那么条件概率p（x|c）涉及x所有属性的联合概率，所需要的样本与属性个数成指数级相关，有限样本远远不够，难以得到条件概率。
朴素：在训练样本中，对于已知类别c，所有属性x相互独立，每个属性独立地对分类结果发生影响。这样就能在有限样本上计算条件概率p（x|c）      。”

### (6)zero 
​      朴素贝叶斯法是基于贝叶斯定理和特征条件独立假设的分类方法。给定训练数据集，首先基于特征条件独立假设学习 输入/输出的联合概率分布，然后根据此模型，对给定的输入x， 利用贝叶斯定理求出后验概率最大的输出y。 
​       在特征条件独立的假设下，对每一个特征通过频率求解其条件概率分布。

### (7)大秋 
朴素贝叶斯法是根据贝叶斯定理的先验概率与后验概率的转换规律，根据只有的统计概率，对未知数据进行分类的方法。所谓“朴素”，是指分析过程中只做最原始最简单的假设，即假设数据的特征之间相互独立，各特征的重要度相同，以简化分析计算过程。

### (8)车车  
朴素贝叶斯法之所以称之为“朴素”，是因为整个形式化过程只做最原始、最简单的假设。
（1）朴素贝叶斯法是典型的生成学习方法。生成方法由训练数据学习联合概率分布P（X,Y）
然后求得后验概率P（Y|X），具体来说，利用训练数据学习条件概率P(X|Y)和先验概率P(Y)的估计，得到联合概率分布：P(X,Y)=P(X|Y)*P(Y)
（2）朴素贝叶斯法的基本假设是条件独立性,这是一个较强的假设。由于这一假设，模型包含的条件概率的数量大为减少，朴素贝叶斯法的学习与预测大为简化。因而朴素贝叶斯法高效，且易于实现。
（3）朴素贝叶斯法利用贝叶斯定理与学到的联合概率模型进行分类预测，将输入x分到后验概率嘴的类y后验概率最大等价于0-1损失函数时的期望风险最小化。

### (9)PoleToWin 
贝叶斯分类是一类分类算法的总称，这类算法均已贝叶斯定理为基础，故统称为贝叶斯分类。P(B|A)=P(A|B)P(B)P(A)该公式最大的优点就是可以忽略AB的联合概率直接求其条件概率分布。
朴素贝叶斯分类：因为它假定所有的特征在数据集中的作用是同样重要和独立的，正如我们所知，这个假设在现实世界中是很不真实的，因此，说是很“朴素的”。

朴素贝叶斯分类是一种十分简单的分类算法，其思想是朴素的，即：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。

### (10)dzysunshin... 
朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。贝叶斯法实现简单，学习与预测的效率都很高，是一种常用的方法。

### (11)谁家那小谁 

首先要了解条件概率公式：p(A|B)=P(B|A)p(A)/p(b)
其中，p(A)称为”先验概率“，即在B事件发生之前，我们对A 事件概率的一个判断，p(A|B)称为”后验概率“，即在B事件发生之后，我们对A事件概率的重新评估（也就是在B发生后，A发生的概率）。为可能性函数，这是一个调整因子。了解条件概率后，我们在现实中经常遇到这样的问题;已知某个条件概率，如何得到两个事件交换后的概率，也就是在已知P（A|B）的情况下求解P（B|A）。贝叶斯定理就为我们打通了从P(A|B)获得P(B|A)的道路。贝叶斯定理为：p(B|A)=p(A|B)p(B)/p(A).朴素贝叶斯对条件概率做了条件独立性的假设。



### (12)Mr.孙 
先验概率：P(Y=1)=5/8,P(Y=-1)=3/8
条件概率：P(X1=1|Y=1)=2/10,P(X1=2|Y=1)=4/10,P(X1=3|Y=1)=4/10
P(X2=S|Y=1)=2/10,P(X2=M|Y=1)=4/10,P(X2=L|Y=1)=4/10
P(X1=1|Y=-1)=3/6,P(X1=2|Y=-1)=2/6,P(X1=3|Y=-1)=1/6
P(X2=S|Y=-1)=3/6,P(X2=M|Y=-1)=2/6,P(X2=L|Y=-1)=1/6
对于给定的x=(2,S)计算：
后验概率分子：P(Y=1)*P(X1=2|Y=1)*P(X2=S|Y=1)=5/8 * 4/10 * 2/10 = 1/20
P(Y=-1)*P(X1=2|Y=-1)*P(X2=S|Y=-1)=3/8 * 2/6 * 3/6=1/16
因为1/16 >1/20，根据后验概率最大原则，得到y=-1.

朴素贝叶斯分类算法的原理：朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布，然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。
之所以称之为“朴素”，是因为整个形式化过程只做最原始、最简单的假设。